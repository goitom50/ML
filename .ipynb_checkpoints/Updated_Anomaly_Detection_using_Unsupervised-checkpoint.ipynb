{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80462d98-aa2a-4df4-8233-050d20556590",
   "metadata": {},
   "source": [
    "             Anomaly Detection using Unsupervised Methods in wood category of MVTec dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87a7a90-21ca-4be8-99b8-1cecfa523125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data\n",
    "def shuffle_data(data):\n",
    "    idx = np.arange(data.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx]\n",
    "\n",
    "train_images = shuffle_data(train_images)\n",
    "test_images = shuffle_data(test_images)\n",
    "test_labels = shuffle_data(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b2a431-af1e-4b49-845b-5393ee2f874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted features data shapes:\n",
      "Train images HOG: (264, 512)\n",
      "Test images HOG: (78, 512)\n"
     ]
    }
   ],
   "source": [
    "# HOG Feature Extraction\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for image in images:\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Resize image to 128x128\n",
    "        resized_image = cv2.resize(gray_image, (128, 128))\n",
    "        \n",
    "        # Extract HOG features\n",
    "        fd, _ = hog(resized_image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True)\n",
    "        hog_features.append(fd)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "# Extract HOG features for training and testing images\n",
    "train_images_hog = extract_hog_features(train_images)\n",
    "test_images_hog = extract_hog_features(test_images)\n",
    "\n",
    "\n",
    "print(\"\\nExtracted features data shapes:\")\n",
    "print(\"Train images HOG:\", train_images_hog.shape)\n",
    "print(\"Test images HOG:\", test_images_hog.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271f31dc-1bac-4508-a277-4024a11c75ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted features data shapes:\n",
      "Extracted_features_of_train_images_vgg16: (264, 8192)\n",
      "Extracted_features_of_test_images_vgg16: (78, 8192)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Initialing compute device (use GPU if available).\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Normalization parameters for VGG16\n",
    "normalization_std = [0.229, 0.224, 0.225]\n",
    "normalization_mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "# Image preprocessing\n",
    "loader = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(128),\n",
    "    transforms.Normalize(mean=normalization_mean, std=normalization_std)\n",
    "])\n",
    "\n",
    "# PyTorch VGG16 Feature Extraction\n",
    "def extract_vgg16_features(images):\n",
    "    model = models.vgg16(weights=VGG16_Weights).features.to(device)\n",
    "    vgg16_features = []\n",
    "    for image in images:\n",
    "        img = loader(image).unsqueeze(0).to(device)\n",
    "        feature = model(img).data.detach().cpu().numpy().flatten()\n",
    "        vgg16_features.append(feature)\n",
    "    return np.array(vgg16_features)\n",
    "\n",
    "# Extract VGG16 features for both normal and anomalous images\n",
    "train_images_vgg16 = extract_vgg16_features(train_images)\n",
    "test_images_vgg16 = extract_vgg16_features(test_images)\n",
    "\n",
    "# Print the extracted features by VGG16\n",
    "print(\"\\nExtracted features data shapes:\")\n",
    "print(\"Extracted_features_of_train_images_vgg16:\", train_images_vgg16.shape)\n",
    "print(\"Extracted_features_of_test_images_vgg16:\", test_images_vgg16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2b9a0b-d591-4e4b-a870-9706f852446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized features of hog data shapes:\n",
      "Normalized train images of HOG: (264, 512)\n",
      "Normalized test images of HOG: (78, 512)\n",
      "\n",
      "Normalized features of VGG16 data shapes:\n",
      "normalized train_images using VGG16: (264, 8192)\n",
      "normalized test_images using VGG16: (78, 8192)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize data\n",
    "def min_max_scaling(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    scaled_data = (data - min_val) / (max_val - min_val)\n",
    "    return scaled_data\n",
    "\n",
    "normalized_train_images = min_max_scaling(train_images_hog)\n",
    "normalized_test_images = min_max_scaling(test_images_hog)\n",
    "\n",
    "normalized_train_images_vgg16 = min_max_scaling(train_images_vgg16)\n",
    "normalized_test_images_vgg16 = min_max_scaling(test_images_vgg16)\n",
    "\n",
    "print(\"\\nNormalized features of hog data shapes:\")\n",
    "print(\"Normalized train images of HOG:\", normalized_train_images.shape)\n",
    "print(\"Normalized test images of HOG:\", normalized_test_images.shape)\n",
    "\n",
    "\n",
    "print(\"\\nNormalized features of VGG16 data shapes:\")\n",
    "print(\"normalized train_images using VGG16:\", normalized_train_images_vgg16.shape)\n",
    "print(\"normalized test_images using VGG16:\", normalized_test_images_vgg16.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37224e13-fafe-42c7-a720-af4322480853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA - Number of components retained: 82\n",
      "\n",
      "Projected data shapes after PCA:\n",
      "Projected train images: (264, 82)\n",
      "Projected test images: (78, 82)\n",
      "\n",
      "Reduction features of VGG16 data shapes:\n",
      "Reduction_train_images_PCA: (264, 110)\n",
      "Reduction_test_images_PCA: (78, 110)\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality reduction using PCA\n",
    "def PCA(train_data, alpha=0.95):\n",
    "    mean = np.mean(train_data, axis=0)\n",
    "    centered_data = train_data - mean\n",
    "    cov_matrix = np.dot(centered_data.T, centered_data)\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eig_values)[::-1]\n",
    "    eig_values = eig_values[idx]\n",
    "    eig_vectors = eig_vectors[:, idx]\n",
    "    total = np.sum(eig_values)\n",
    "    k = 0\n",
    "    var = 0\n",
    "    while var / total < alpha:\n",
    "        var += eig_values[k]\n",
    "        k += 1\n",
    "    eig_vectors = eig_vectors[:, :k]\n",
    "    return eig_vectors, mean\n",
    "\n",
    "# Apply PCA on normalized training data\n",
    "pca_components, mean = PCA(normalized_train_images)\n",
    "\n",
    "# Project the normalized training and testing data onto the PCA components\n",
    "train_projected_pca = np.dot(normalized_train_images - mean, pca_components)\n",
    "test_projected_pca = np.dot(normalized_test_images - mean, pca_components)\n",
    "print(f\"\\nPCA - Number of components retained: {pca_components.shape[1]}\")\n",
    "\n",
    "print(\"\\nProjected data shapes after PCA:\")\n",
    "print(\"Projected train images:\", train_projected_pca.shape)\n",
    "print(\"Projected test images:\", test_projected_pca.shape)\n",
    "\n",
    "\n",
    "# Perform PCA on VGG16 features\n",
    "space_pca_vgg16, mean_pca_vgg16 = PCA(normalized_train_images_vgg16)\n",
    "train_projected_pca_vgg16 = np.dot(normalized_train_images_vgg16 - mean_pca_vgg16, space_pca_vgg16)\n",
    "test_projected_pca_vgg16 = np.dot(normalized_test_images_vgg16 - mean_pca_vgg16, space_pca_vgg16)\n",
    "\n",
    "print(\"\\nReduction features of VGG16 data shapes:\")\n",
    "print(\"Reduction_train_images_PCA:\", train_projected_pca_vgg16.shape)\n",
    "print(\"Reduction_test_images_PCA:\", test_projected_pca_vgg16.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bb3160-7594-421d-a105-dfc5dd168206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom implementation of K-Means with convergence iteration tracking\n",
    "def k_means(X, n_clusters, max_iters=100, tol=1e-4):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = X[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "    centroid_history = [centroids.copy()]  # Track centroid history\n",
    "    for iter_ in range(max_iters):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
    "        centroid_history.append(new_centroids.copy())  # Track new centroids\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return labels, centroids, iter_ + 1, centroid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfc35f-ccb4-4575-b8b9-dee72b96e127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Function to load images and convert to grayscale\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            images.append(gray)\n",
    "    return images\n",
    "\n",
    "# Function to extract HOG features from images\n",
    "def extract_hog_features(images):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    features = [hog.compute(image).flatten() for image in images]\n",
    "    return np.array(features)\n",
    "\n",
    "# Load normal images and extract features\n",
    "normal_images = load_images_from_folder('path_to_normal_images')\n",
    "normal_features = extract_hog_features(normal_images)\n",
    "\n",
    "# Train k-means on normal features\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(normal_features)\n",
    "\n",
    "# Load mixed images (normal + anomalies) and extract features\n",
    "mixed_images = load_images_from_folder('path_to_mixed_images')\n",
    "mixed_features = extract_hog_features(mixed_images)\n",
    "\n",
    "# Predict the nearest cluster for each mixed feature\n",
    "closest_clusters, distances = pairwise_distances_argmin_min(mixed_features, kmeans.cluster_centers_)\n",
    "\n",
    "# Define a threshold for anomalies\n",
    "threshold = np.percentile(distances, 95)  # 95th percentile as an example\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = distances > threshold\n",
    "\n",
    "# Print results\n",
    "for i, is_anomaly in enumerate(anomalies):\n",
    "    if is_anomaly:\n",
    "        print(f\"Image {i} is an anomaly.\")\n",
    "    else:\n",
    "        print(f\"Image {i} is normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# DBSCAN for anomaly detection\n",
    "def dbscan_anomaly_detection(normal_features, mixed_features, eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(normal_features)\n",
    "    mixed_labels = dbscan.fit_predict(mixed_features)\n",
    "    anomalies = mixed_labels == -1\n",
    "    return anomalies\n",
    "\n",
    "# DBSCAN Anomaly Detection\n",
    "anomalies_dbscan = dbscan_anomaly_detection(normal_features, mixed_features)\n",
    "\n",
    "# Print results for DBSCAN\n",
    "print(\"DBSCAN Results:\")\n",
    "for i, is_anomaly in enumerate(anomalies_dbscan):\n",
    "    if is_anomaly:\n",
    "        print(f\"Image {i} is an anomaly.\")\n",
    "    else:\n",
    "        print(f\"Image {i} is normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# GMM for anomaly detection\n",
    "def gmm_anomaly_detection(normal_features, mixed_features, n_components=5, threshold=0.01):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=0).fit(normal_features)\n",
    "    log_likelihood = gmm.score_samples(mixed_features)\n",
    "    anomalies = log_likelihood < np.log(threshold)\n",
    "    return anomalies\n",
    "\n",
    "# GMM Anomaly Detection\n",
    "anomalies_gmm = gmm_anomaly_detection(normal_features, mixed_features)\n",
    "\n",
    "# Print results for GMM\n",
    "print(\"GMM Results:\")\n",
    "for i, is_anomaly in enumerate(anomalies_gmm):\n",
    "    if is_anomaly:\n",
    "        print(f\"Image {i} is an anomaly.\")\n",
    "    else:\n",
    "        print(f\"Image {i} is normal.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
