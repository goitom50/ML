{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80462d98-aa2a-4df4-8233-050d20556590",
   "metadata": {},
   "source": [
    "             Anomaly Detection using Unsupervised Methods in Screw category of MVTec dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87a7a90-21ca-4be8-99b8-1cecfa523125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normal images from ML/dataset/MVTec\\screw\\train\\good...\n",
      "Loaded 320 normal images from ML/dataset/MVTec\\screw\\train\\good\n",
      "Loading images from ML/dataset/MVTec\\screw\\test\\good...\n",
      "Loaded 41 images from ML/dataset/MVTec\\screw\\test\\good\n",
      "Loading images from ML/dataset/MVTec\\screw\\test\\manipulated_front...\n",
      "Loaded 24 images from ML/dataset/MVTec\\screw\\test\\manipulated_front\n",
      "Loading images from ML/dataset/MVTec\\screw\\test\\scratch_head...\n",
      "Loaded 24 images from ML/dataset/MVTec\\screw\\test\\scratch_head\n",
      "Loading images from ML/dataset/MVTec\\screw\\test\\scratch_neck...\n",
      "Loaded 25 images from ML/dataset/MVTec\\screw\\test\\scratch_neck\n",
      "Loading images from ML/dataset/MVTec\\screw\\test\\thread_side...\n",
      "Loaded 23 images from ML/dataset/MVTec\\screw\\test\\thread_side\n",
      "Loading images from ML/dataset/MVTec\\screw\\test\\thread_top...\n",
      "Loaded 23 images from ML/dataset/MVTec\\screw\\test\\thread_top\n",
      "Total normal images loaded for training: 320\n",
      "Total images loaded for testing: 160\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to load MVTec images from a specific folder structure\n",
    "def load_mvtec_images(path, size=(128, 128), color=True):\n",
    "    images = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.png') or file.endswith('.jpg'):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    if color:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    else:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                    img = cv2.resize(img, size)\n",
    "                    images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load data for a given category in an unsupervised manner\n",
    "def load_data(category, dataset_root='ML/dataset/MVTec'):\n",
    "    category_path = os.path.join(dataset_root, category)\n",
    "    train_good_path = os.path.join(category_path, 'train', 'good')\n",
    "    test_path = os.path.join(category_path, 'test')\n",
    "    # Load normal images from train/good\n",
    "    print(f\"Loading normal images from {train_good_path}...\")\n",
    "    train_normal_images = load_mvtec_images(train_good_path)\n",
    "    print(f\"Loaded {len(train_normal_images)} normal images from {train_good_path}\")\n",
    "    # Load all images from test folder\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for subfolder in os.listdir(test_path):\n",
    "        subfolder_path = os.path.join(test_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            label = 0 if subfolder == 'good' else 1\n",
    "            print(f\"Loading images from {subfolder_path}...\")\n",
    "            images = load_mvtec_images(subfolder_path)\n",
    "            test_images.append(images)\n",
    "            test_labels.append(np.full(images.shape[0], label))\n",
    "            print(f\"Loaded {images.shape[0]} images from {subfolder_path}\")\n",
    "    test_images = np.concatenate(test_images)\n",
    "    test_labels = np.concatenate(test_labels).reshape(-1, 1)\n",
    "    print(f\"Total normal images loaded for training: {len(train_normal_images)}\")\n",
    "    print(f\"Total images loaded for testing: {len(test_images)}\")\n",
    "    return train_normal_images, test_images, test_labels\n",
    "\n",
    "# Just Change the category [wood, pill, screw, bottle, grid] that you want to train the model, Default is \"wood\"\n",
    "\n",
    "category = 'screw'\n",
    "train_images, test_images, test_labels = load_data(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b2a431-af1e-4b49-845b-5393ee2f874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted features data shapes:\n",
      "Train images HOG: (320, 512)\n",
      "Test images HOG: (160, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Shuffle training data\n",
    "def shuffle_data(data):\n",
    "    idx = np.arange(data.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx]\n",
    "\n",
    "train_images = shuffle_data(train_images)\n",
    "test_images = shuffle_data(test_images)\n",
    "test_labels = shuffle_data(test_labels)\n",
    "\n",
    "# HOG Feature Extraction\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for image in images:\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Resize image to 128x128\n",
    "        resized_image = cv2.resize(gray_image, (128, 128))\n",
    "        \n",
    "        # Extract HOG features\n",
    "        fd, _ = hog(resized_image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True)\n",
    "        hog_features.append(fd)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "# Extract HOG features for training and testing images\n",
    "train_images_hog = extract_hog_features(train_images)\n",
    "test_images_hog = extract_hog_features(test_images)\n",
    "\n",
    "\n",
    "print(\"\\nExtracted features data shapes:\")\n",
    "print(\"Train images HOG:\", train_images_hog.shape)\n",
    "print(\"Test images HOG:\", test_images_hog.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271f31dc-1bac-4508-a277-4024a11c75ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted features data shapes:\n",
      "Extracted_features_of_train_images_vgg16: (320, 8192)\n",
      "Extracted_features_of_test_images_vgg16: (160, 8192)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Initialing compute device (use GPU if available).\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Normalization parameters for VGG16\n",
    "normalization_std = [0.229, 0.224, 0.225]\n",
    "normalization_mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "# Image preprocessing\n",
    "loader = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(128),\n",
    "    transforms.Normalize(mean=normalization_mean, std=normalization_std)\n",
    "])\n",
    "\n",
    "# PyTorch VGG16 Feature Extraction\n",
    "def extract_vgg16_features(images):\n",
    "    model = models.vgg16(weights=VGG16_Weights).features.to(device)\n",
    "    vgg16_features = []\n",
    "    for image in images:\n",
    "        img = loader(image).unsqueeze(0).to(device)\n",
    "        feature = model(img).data.detach().cpu().numpy().flatten()\n",
    "        vgg16_features.append(feature)\n",
    "    return np.array(vgg16_features)\n",
    "\n",
    "# Extract VGG16 features for both normal and anomalous images\n",
    "train_images_vgg16 = extract_vgg16_features(train_images)\n",
    "test_images_vgg16 = extract_vgg16_features(test_images)\n",
    "\n",
    "# Print the extracted features by VGG16\n",
    "print(\"\\nExtracted features data shapes:\")\n",
    "print(\"Extracted_features_of_train_images_vgg16:\", train_images_vgg16.shape)\n",
    "print(\"Extracted_features_of_test_images_vgg16:\", test_images_vgg16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2b9a0b-d591-4e4b-a870-9706f852446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized features of hog data shapes:\n",
      "Normalized train images of HOG: (320, 512)\n",
      "Normalized test images of HOG: (160, 512)\n",
      "\n",
      "Normalized features of VGG16 data shapes:\n",
      "normalized train_images using VGG16: (320, 8192)\n",
      "normalized test_images using VGG16: (160, 8192)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize data\n",
    "def min_max_scaling(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    scaled_data = (data - min_val) / (max_val - min_val)\n",
    "    return scaled_data\n",
    "\n",
    "normalized_train_images = min_max_scaling(train_images_hog)\n",
    "normalized_test_images = min_max_scaling(test_images_hog)\n",
    "\n",
    "normalized_train_images_vgg16 = min_max_scaling(train_images_vgg16)\n",
    "normalized_test_images_vgg16 = min_max_scaling(test_images_vgg16)\n",
    "\n",
    "print(\"\\nNormalized features of hog data shapes:\")\n",
    "print(\"Normalized train images of HOG:\", normalized_train_images.shape)\n",
    "print(\"Normalized test images of HOG:\", normalized_test_images.shape)\n",
    "\n",
    "\n",
    "print(\"\\nNormalized features of VGG16 data shapes:\")\n",
    "print(\"normalized train_images using VGG16:\", normalized_train_images_vgg16.shape)\n",
    "print(\"normalized test_images using VGG16:\", normalized_test_images_vgg16.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37224e13-fafe-42c7-a720-af4322480853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA - Number of components retained: 127\n",
      "\n",
      "Projected data shapes after PCA:\n",
      "Projected train images: (320, 127)\n",
      "Projected test images: (160, 127)\n",
      "\n",
      "Reduction features of VGG16 data shapes:\n",
      "Reduction_train_images_PCA: (320, 168)\n",
      "Reduction_test_images_PCA: (160, 168)\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality reduction using PCA\n",
    "def PCA(train_data, alpha=0.95):\n",
    "    mean = np.mean(train_data, axis=0)\n",
    "    centered_data = train_data - mean\n",
    "    cov_matrix = np.dot(centered_data.T, centered_data)\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eig_values)[::-1]\n",
    "    eig_values = eig_values[idx]\n",
    "    eig_vectors = eig_vectors[:, idx]\n",
    "    total = np.sum(eig_values)\n",
    "    k = 0\n",
    "    var = 0\n",
    "    while var / total < alpha:\n",
    "        var += eig_values[k]\n",
    "        k += 1\n",
    "    eig_vectors = eig_vectors[:, :k]\n",
    "    return eig_vectors, mean\n",
    "\n",
    "# Apply PCA on normalized training data\n",
    "pca_components, mean = PCA(normalized_train_images)\n",
    "\n",
    "# Project the normalized training and testing data onto the PCA components\n",
    "train_projected_pca = np.dot(normalized_train_images - mean, pca_components)\n",
    "test_projected_pca = np.dot(normalized_test_images - mean, pca_components)\n",
    "print(f\"\\nPCA - Number of components retained: {pca_components.shape[1]}\")\n",
    "\n",
    "print(\"\\nProjected data shapes after PCA:\")\n",
    "print(\"Projected train images:\", train_projected_pca.shape)\n",
    "print(\"Projected test images:\", test_projected_pca.shape)\n",
    "\n",
    "\n",
    "# Perform PCA on VGG16 features\n",
    "space_pca_vgg16, mean_pca_vgg16 = PCA(normalized_train_images_vgg16)\n",
    "train_projected_pca_vgg16 = np.dot(normalized_train_images_vgg16 - mean_pca_vgg16, space_pca_vgg16)\n",
    "test_projected_pca_vgg16 = np.dot(normalized_test_images_vgg16 - mean_pca_vgg16, space_pca_vgg16)\n",
    "\n",
    "print(\"\\nReduction features of VGG16 data shapes:\")\n",
    "print(\"Reduction_train_images_PCA:\", train_projected_pca_vgg16.shape)\n",
    "print(\"Reduction_test_images_PCA:\", test_projected_pca_vgg16.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d778b353-ca3c-4d1d-9468-0646ade02c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(X, n_clusters, max_iters=100, tol=1e-4):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = X[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "    centroid_history = [centroids.copy()]  # Track centroid history\n",
    "    for iter_ in range(max_iters):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
    "        centroid_history.append(new_centroids.copy())  # Track new centroids\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    # Calculate distances to nearest centroids\n",
    "    distances_to_centroids = np.min(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1)\n",
    "    \n",
    "    return labels, centroids, iter_ + 1, centroid_history, distances_to_centroids\n",
    "\n",
    "def k_means_with_cv_threshold(X, n_clusters, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_threshold = 0\n",
    "    best_score = -1\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        \n",
    "        # Fit K-means on training data\n",
    "        _, centroids, _, _, _ = k_means(X_train, n_clusters)\n",
    "        \n",
    "        # Calculate distances for validation set\n",
    "        distances_val = np.min(np.linalg.norm(X_val[:, np.newaxis] - centroids, axis=2), axis=1)\n",
    "        \n",
    "        # Try different thresholds\n",
    "        for threshold in np.percentile(distances_val, range(50, 100, 5)):\n",
    "            predictions = (distances_val > threshold).astype(int)\n",
    "            score = f1_score(np.ones_like(predictions), predictions, average='macro')\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "\n",
    "    # Fit final model on all data\n",
    "    labels, centroids, _, _, distances = k_means(X, n_clusters)\n",
    "    \n",
    "    return centroids, best_threshold, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9541a54-ab45-4673-ab92-e76c0dd25451",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"DBSCAN\", DBSCAN(), {'eps': [0.1, 0.5, 1], 'min_samples': [5, 10, 20]}),\n",
    "    (\"Gaussian Mixture Model\", GaussianMixture(n_components=2), {}),  \n",
    "    (\"K-Means\", KMeans(n_clusters=2), {})  \n",
    "]\n",
    "\n",
    "# Initialize lists to store classifier names, training accuracies, test accuracies, and F1-scores\n",
    "classifier_names = []\n",
    "test_accuracies = []\n",
    "f1_scores = []\n",
    "hyperparameters = []\n",
    "\n",
    "X_for_training = [normalized_train_images, train_projected_pca]\n",
    "X_for_test = [normalized_test_images, test_projected_pca]\n",
    "\n",
    "X_for_training_vgg16 = [normalized_train_images_vgg16, train_projected_pca_vgg16]\n",
    "X_for_test_vgg16 = [normalized_test_images_vgg16, test_projected_pca_vgg16]\n",
    "\n",
    "\n",
    "text = ['With Out PCA', 'With PCA']\n",
    "\n",
    "def evaluate_classifier(name, classifier, param_grid, X_train, X_test, y_test, feature_text):\n",
    "    best_params = None\n",
    "    best_score = -1\n",
    "    \n",
    "    if isinstance(classifier, KMeans):\n",
    "        centroids, threshold, distances_train = k_means_with_cv_threshold(X_train, n_clusters=2)\n",
    "        distances_test = np.min(np.linalg.norm(X_test[:, np.newaxis] - centroids, axis=2), axis=1)\n",
    "        predictions = (distances_test > threshold).astype(int)\n",
    "        best_params = {\"n_clusters\": 2, \"threshold\": threshold}\n",
    "    elif isinstance(classifier, GaussianMixture):\n",
    "        classifier.fit(X_train)\n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        anomaly_label = 0 if np.sum(train_predictions == 0) < np.sum(train_predictions == 1) else 1\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        predictions = (test_predictions == anomaly_label).astype(int)\n",
    "        best_params = {\"n_components\": 2}\n",
    "    elif isinstance(classifier, DBSCAN):\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            classifier.set_params(**params)\n",
    "            predictions = classifier.fit_predict(X_test)\n",
    "            predictions = np.where(predictions == -1, 1, 0)\n",
    "            score = f1_score(y_test, predictions, average='macro')\n",
    "            if score > best_score:\n",
    "                best_params = params\n",
    "                best_score = score\n",
    "        classifier.set_params(**best_params)\n",
    "        predictions = classifier.fit_predict(X_test)\n",
    "        predictions = np.where(predictions == -1, 1, 0)\n",
    "    \n",
    "    test_score = accuracy_score(y_test, predictions)\n",
    "    current_f1_score = f1_score(y_test, predictions, average='macro')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{feature_text} - {name}\")\n",
    "    print(f\"Testing Accuracy: {test_score * 100:.4f}\")\n",
    "    print(f\"F1 Score: {current_f1_score:.4f}\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    conmat = confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conmat, annot=True, fmt=\".1f\", linewidth=1, cmap=\"crest\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"Confusion Matrix for {name} - {feature_text}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Append results to lists\n",
    "    classifier_names.append(name)\n",
    "    test_accuracies.append(test_score)\n",
    "    f1_scores.append(current_f1_score)\n",
    "    hyperparameters.append(str(best_params))\n",
    "\n",
    "    return test_score, current_f1_score, best_params\n",
    "\n",
    "# Evaluate classifiers\n",
    "for name, classifier, param_grid in classifiers:\n",
    "    for i in range(len(text)):\n",
    "        evaluate_classifier(name, classifier, param_grid, X_for_training[i], X_for_test[i], test_labels, f\"{text[i]} (HOG)\")\n",
    "\n",
    "# Print the best model information\n",
    "best_model_info = {'name': None, 'feature_extraction': None, 'f1_score': 0, 'hyperparameters': None}\n",
    "for name, feature_extraction, f1, params in zip(classifier_names, [text[i % len(text)] for i in range(len(classifier_names))], f1_scores, hyperparameters):\n",
    "    if f1 > best_model_info['f1_score']:\n",
    "        best_model_info['name'] = name\n",
    "        best_model_info['feature_extraction'] = feature_extraction\n",
    "        best_model_info['f1_score'] = f1\n",
    "        best_model_info['hyperparameters'] = params\n",
    "print(f\"\\nThe Best model is: {best_model_info['name']} with {best_model_info['feature_extraction']} method, having an F1 score of {best_model_info['f1_score']:.4f}\")\n",
    "print(f\"Hyperparameters: {best_model_info['hyperparameters']}\\n\\n\")\n",
    "\n",
    "\n",
    "# Evaluate classifiers for VGG16 features\n",
    "for name, classifier, param_grid in classifiers:\n",
    "    for i in range(len(text)):\n",
    "        evaluate_classifier(name, classifier, param_grid, X_for_training_vgg16[i], X_for_test_vgg16[i], test_labels, f\"{text[i]} (VGG16)\")\n",
    "\n",
    "# Print the best model information (including VGG16)\n",
    "best_model_info = {'name': None, 'feature_extraction': None, 'f1_score': 0, 'hyperparameters': None}\n",
    "for name, feature_extraction, f1, params in zip(classifier_names, [f\"{text[i % len(text)]} ({('VGG16' if i >= len(text) else 'HOG')})\" for i in range(len(classifier_names))], f1_scores, hyperparameters):\n",
    "    if f1 > best_model_info['f1_score']:\n",
    "        best_model_info['name'] = name\n",
    "        best_model_info['feature_extraction'] = feature_extraction\n",
    "        best_model_info['f1_score'] = f1\n",
    "        best_model_info['hyperparameters'] = params\n",
    "\n",
    "print(f\"\\nThe Best model is: {best_model_info['name']} with {best_model_info['feature_extraction']} method, having an F1 score of {best_model_info['f1_score']:.4f}\")\n",
    "print(f\"Hyperparameters: {best_model_info['hyperparameters']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753711f7-3b2c-435b-baf8-e608f39e00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7c9c0-4d32-4968-9687-54cc92340330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35888e-9c3b-48c8-bcc7-6a40e1f3b544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74f6a-7fc3-4b65-8ef2-3a1db7ffeed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f80a2-ce94-4a15-8509-56c2a1414b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a4547-730a-4d53-bd14-114547ae5dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f5d3f-76fb-49c0-935e-85d90d6130f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
